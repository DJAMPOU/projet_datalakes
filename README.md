# ğŸš– Big Data Pipeline - NYC Taxi Trip Duration

## ğŸ“Œ Description du projet  
Ce projet met en place une **pipeline Big Data complÃ¨te** pour le traitement et lâ€™analyse des donnÃ©es de la compÃ©tition Kaggle **NYC Taxi Trip Duration**.  
L'objectif est de passer les donnÃ©es brutes Ã  travers plusieurs Ã©tapes de transformation avant de les stocker dans une base de donnÃ©es SQL structurÃ©e.  

---

## ğŸ”„ Architecture de la Pipeline  
1. **Ingestion des donnÃ©es (Raw Layer - S3)**  
   - Les donnÃ©es brutes sont tÃ©lÃ©chargÃ©es depuis l'API Kaggle.  
   - Elles sont stockÃ©es dans un **bucket S3**.  

2. **Transformation intermÃ©diaire (Silver Layer - MySQL)**  
   - Nettoyage des donnÃ©es : suppression des valeurs nulles, formatage des dates, etc.  
   - Insertion des donnÃ©es dans une **table MySQL**.  

3. **Transformation finale (Gold Layer - MySQL)**  
   - SÃ©lection des colonnes importantes.  
   - Calcul des distances de trajet Ã  partir des coordonnÃ©es GPS.  
   - Stockage final dans une **table MySQL optimisÃ©e**.  

---

## ğŸš€ Installation et ExÃ©cution  

### 1ï¸âƒ£ Cloner le projet  
```bash
git clone https://github.com/DJAMPOU/projet_datalakes.git -b master
cd projet_datalakes
```
### 2ï¸âƒ£ Lancer le projet avec le script
Le script start_airflow.sh va :
- âœ… CrÃ©er les rÃ©pertoires nÃ©cessaires (logs, plugins).
- âœ… DÃ©ployer le Docker Compose qui lance tous les conteneurs nÃ©cessaires.
- âœ… Afficher lâ€™IP, le port, le user et le mot de passe pour accÃ©der Ã  lâ€™interface Airflow.

```bash
chmod +x script.sh
./script.sh
```

### 3ï¸âƒ£ AccÃ©der Ã  lâ€™interface Airflow
Une fois le script exÃ©cutÃ©, notez les informations affichÃ©es en console :

URL du Web Server : http://<IP>:8080
User : airflow
Mot de passe : airflow
Connectez-vous et lancez la pipeline pour traiter les donnÃ©es. ğŸš€

## ğŸ›  Technologies utilisÃ©es
* Airflow : Orchestration des tÃ¢ches
* Docker & Docker Compose : DÃ©ploiement des services
* Kaggle API : Ingestion des donnÃ©es
* MySQL : Stockage des donnÃ©es transformÃ©es
* Localstack : Simuler un s3 en local
* Python : Manipulation et transformation des donnÃ©es
  
## ğŸ“œ Organisation des fichiers
- ğŸ“‚ dags/ â†’ Contient les fichiers DAGs Airflow pour lâ€™orchestration.
- ğŸ“‚ app/ â†’ Contient les scripts dâ€™ingestion et de transformation.
- ğŸ“„ docker-compose.yml â†’ Fichier Docker Compose pour dÃ©ployer tous les services.
- ğŸ“„ script.sh â†’ Script pour lancer et configurer le projet.

## âœ¨ Contributeurs
- ğŸ‘¤ DJAMPOU Pedro Le Prince - DÃ©veloppeur Big Data
- ğŸ“§ Contact : pierrewalter24@gmail.com
